{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "import torch\n",
    "from PIL import Image, ImageDraw\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from langchain_openai.chat_models import AzureChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema import HumanMessage\n",
    "import base64\n",
    "import re\n",
    "import asyncio\n",
    "from fastapi import WebSocket\n",
    "import json\n",
    "#from ultralytics import YOLO\n",
    "\n",
    "# Set Azure OpenAI environment variables\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "os.environ[\"OPENAI_API_BASE\"] = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2023-03-15-preview\"\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def get_all_images_from_dir(path_to_dir):\n",
    "    #with locks.file_lock:\n",
    "    regex = re.compile('.*\\.(jpe?g|png)$')\n",
    "    f_matches = []\n",
    "\n",
    "    for root, dirs, files in os.walk(path_to_dir):\n",
    "        for file in files:\n",
    "            if regex.match(file):\n",
    "                f_matches.append(file)\n",
    "    print(f_matches)\n",
    "    return f_matches\n",
    "\n",
    "def generate_prompt_from_images(images, sys_prompt, instructions):\n",
    "    m_intructions = [(\"system\", sys_prompt)]\n",
    "    for image in images:\n",
    "        image_data = encode_image(image)\n",
    "        m_intructions.append(\n",
    "            HumanMessage(\n",
    "                    content=[\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"},\n",
    "                        },\n",
    "                    ],\n",
    "                )\n",
    "            )\n",
    "    m_intructions.append(HumanMessage(content=[{\"type\": \"text\", \"text\": instructions}])),\n",
    "    return ChatPromptTemplate.from_messages(m_intructions)\n",
    "\n",
    "def run_analyzer(chat):\n",
    "\n",
    "    images = get_all_images_from_dir(\"./\")\n",
    "    image_data = encode_image(images[0])\n",
    "\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Analyze the given images for signs of danger, and describe the source of it\"),\n",
    "        HumanMessage(\n",
    "        content=[\n",
    "            {\"type\": \"text\", \"text\": \"describe this image\"},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": \"data:image/jpeg;base64,{image_data}\"},\n",
    "            },\n",
    "        ],\n",
    "        )\n",
    "    ])\n",
    "    \"\"\"\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "                        You must only analyze the images for danger\n",
    "                        You must consider things like open fires, step hazards, and things of that nature things of IMMEDIATE DANGER\n",
    "                        Tou must consider things like potential flames, hazardous materials, train tracks, and other potential dangers as POTENTIAL DANGER\n",
    "                        If you find nothing that can be considered dangerous on the image, you must consider the danger level as LOW DANGER\n",
    "                        You MUST only respond in the format of a valid json, as formatted below. DO NOT add json to the start of the message:\n",
    "                        {{\n",
    "                            \"type\" : \"danger_analysis\"\n",
    "                            \"danger_level\": \"{{level of danger detected on the image}}\",\n",
    "                            \"danger_source\": \"{{the source of danger, if detected. if none are detected, fill with NoDangerSources}}\"\n",
    "                        }}\n",
    "                    \"\"\"\n",
    "    instruction_prompt = \"Analyze all the images. Give me a response for each one of the images\"\n",
    "\n",
    "    prompt = generate_prompt_from_images(images, system_prompt, instruction_prompt)\n",
    "\n",
    "    # Create a human message\n",
    "    #response = chat.invoke([message])\n",
    "\n",
    "    #print(response.content)\n",
    "\n",
    "    print(\"##################\")\n",
    "    output_parser = StrOutputParser()\n",
    "    chain = prompt | chat | output_parser\n",
    "    #response = chain.invoke({\"input\": message})\n",
    "    response = chain.invoke({})\n",
    "    print(response)\n",
    "\n",
    "def get_classes_from_prompt_dino(chat):\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"\n",
    "                Your task is to aid in the selection of the adequate classes for a vision detection program. \n",
    "                You will recieve a brief description of what the user wants to detect, and what is relevant to him\n",
    "                You will respond only with a sentence, with each object that will be detected.\n",
    "                For example, the user input is: I am a birdwatcher, and I am on the lookout for birds and other animals that live in the area, such as bears.\n",
    "                The response will be: A bird, a bear.\n",
    "                Do not limit yourself to what is on the prompt. Given context, add anything whitch may be relevant to the user.\n",
    "             \"\"\"),\n",
    "            (\"user\", \"{input}\")\n",
    "            ])\n",
    "\n",
    "    input_str = input(\"Describe what you want GroundingDINO to identify on the scene. \")\n",
    "\n",
    "    output_parser = StrOutputParser()\n",
    "    chain = prompt | chat | output_parser\n",
    "    response = chain.invoke(\n",
    "            {\n",
    "                \"input\":input_str\n",
    "            })\n",
    "    print(\"Classes to be identified by grounding dino: \", response)\n",
    "    return response\n",
    "\n",
    "\n",
    "chat = AzureChatOpenAI(\n",
    "    deployment_name=\"grad-eng\",  # Your deployment name\n",
    "    openai_api_version=\"2023-03-15-preview\",\n",
    "    model=\"gpt-4o\"\n",
    ")\n",
    "#run_analyzer(chat)\n",
    "text = get_classes_from_prompt_dino(chat)\n",
    "# print(\"AAAAAA\")\n",
    "\n",
    "\n",
    "\n",
    "model_id = \"IDEA-Research/grounding-dino-tiny\"\n",
    "device = \"cuda\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(device)\n",
    "\n",
    "#image_dogs_url = \"https://www.lonetreevet.com/blog/wp-content/uploads/2019/05/iStock-688041916.jpg\"\n",
    "# image_url = \"https://t4.ftcdn.net/jpg/03/09/37/33/360_F_309373366_ypYQ67CJREwZC9ma0mw94NYcc55mrdtf.jpg\"\n",
    "image_url = \"https://cdn-ckgki.nitrocdn.com/eIjtlqSrzAKXFrsHSjkfXOrmttOUeOlc/assets/images/optimized/rev-69b956e/esub.com/wp-content/uploads/2020/10/shutterstock_1180341814-e1601583379631.jpg\"\n",
    "#image_url = \"https://www.sociallifeproject.org/content/images/2022/05/Democratize-Streets-as-Places-for-People-1-1.jpeg\"\n",
    "image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "# Check for cats and remote controls\n",
    "#text = \"A car. A pedestrian\"\n",
    "\n",
    "inputs = processor(images=image, text=text, return_tensors=\"pt\").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "results = processor.post_process_grounded_object_detection(\n",
    "    outputs,\n",
    "    inputs.input_ids,\n",
    "    box_threshold=0.3,\n",
    "    text_threshold=0.3,\n",
    "    target_sizes=[image.size[::-1]]\n",
    ")\n",
    "\n",
    "image = np.array(image)  # Uncomment if image is still a PIL image\n",
    "image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "print(f\"model results{results}\")\n",
    "for result in results:\n",
    "    boxes = result[\"boxes\"].cpu().tolist()  # Convert tensor to list and move to CPU\n",
    "    labels = result[\"labels\"]\n",
    "    # Draw the rectangle for each box\n",
    "    for i in range(len(boxes)):\n",
    "        box = boxes[i]\n",
    "        label = labels[i]\n",
    "        # OpenCV uses (x1, y1, x2, y2) for rectangle\n",
    "        cv2.rectangle(image, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (0, 255, 0), 2)\n",
    "        cv2.putText(image, label,(int(box[0]), int(box[1] - 10)),0,0.3,(0,255,0))\n",
    "\n",
    "# Display the image using OpenCV\n",
    "cv2.imshow('Image', image)\n",
    "cv2.waitKey(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
